{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6549af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "!pip install -U datasets tiktoken tqdm numpy torch matplotlib huggingface_hub\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization implementation\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import tiktoken \n",
    "import math\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot_token = enc.eot_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fda9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_bpe(sample):\n",
    "    text_data = sample.get('text', '')\n",
    "    if not isinstance(text_data, str): text_data = \"\"\n",
    "    ids = enc.encode_ordinary(text_data)\n",
    "    ids.append(eot_token)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "combined_data_dir = config.data_dir\n",
    "os.makedirs(combined_data_dir, exist_ok=True)\n",
    "train_filename = os.path.join(combined_data_dir, 'train_combined.bin')\n",
    "val_filename = os.path.join(combined_data_dir, 'val_combined.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer with data splits..\n",
    "if os.path.exists(train_filename) and os.path.exists(val_filename):\n",
    "    print(f\"Combined .bin files already exist in {combined_data_dir}. Skipping processing.\")\n",
    "\n",
    "else:\n",
    "    ts_df = load_dataset(\"roneneldan/TinyStories\")\n",
    "    bc_df = load_dataset(\"rojagtap/bookcorpus\", split='train')\n",
    "    \n",
    "    ts_split = ts_df['train'].train_test_split(test_size=0.01, seed=42)\n",
    "    ts_train_df = ts_split['train']\n",
    "    ts_val_df = ts_split['test']\n",
    "    \n",
    "    bc_split = bc_df.train_test_split(test_size=0.01, seed=42)\n",
    "    bc_train_df = bc_split['train']\n",
    "    bc_val_df = bc_split['test']\n",
    "\n",
    "    dataset_splits = {\n",
    "        'train': {'ts': ts_train_df, 'bc': bc_train_df},\n",
    "        'validation': {'ts': ts_val_df, 'bc': bc_val_df}\n",
    "    }\n",
    "\n",
    "    for split in ['train', 'validation']:\n",
    "        filename = train_filename if split == 'train' else val_filename\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "        tokenized_ts = dataset_splits[split]['ts'].map(\n",
    "            processing_bpe, remove_columns=['text'],\n",
    "            desc=f\"Tokenizing TinyStories {split}\", num_proc=config.num_proc\n",
    "        )\n",
    "        \n",
    "        tokenized_bc = dataset_splits[split]['bc'].map(\n",
    "            processing_bpe, remove_columns=['text'],\n",
    "            desc=f\"Tokenizing BookCorpus {split}\", num_proc=config.num_proc\n",
    "        )\n",
    "\n",
    "        ts_len = np.sum(tokenized_ts['len'], dtype=np.uint64)\n",
    "        bc_len = np.sum(tokenized_bc['len'], dtype=np.uint64)\n",
    "        arr_len = ts_len + bc_len\n",
    "\n",
    "        if arr_len == 0:\n",
    "            print(f\"Warning: No tokens found for combined {split} split. Skipping.\"); continue\n",
    "            \n",
    "        print(f\"Total tokens for {split}: {arr_len:,} (TinyStories: {ts_len:,}, BookCorpus: {bc_len:,})\")\n",
    "        dtype_np = np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype_np, mode='w+', shape=(arr_len,))\n",
    "        \n",
    "        print(f\"Writing {ts_len:,} TinyStories tokens to {filename}...\")\n",
    "        idx = 0\n",
    "        write_batch_size = 1000 # Docs per chunk\n",
    "        \n",
    "        total_samples_ts = len(tokenized_ts)\n",
    "        num_write_batches_ts = math.ceil(total_samples_ts / write_batch_size)\n",
    "        for i in tqdm(range(num_write_batches_ts), desc=f\"Writing TinyStories {split}\"):\n",
    "            start = i * write_batch_size; end = min((i + 1) * write_batch_size, total_samples_ts)\n",
    "            chunk = tokenized_ts.select(range(start, end))\n",
    "            try:\n",
    "                all_ids_in_chunk = [item['ids'] for item in chunk if item['ids']]\n",
    "                if not all_ids_in_chunk: continue\n",
    "                arr_batch = np.concatenate(all_ids_in_chunk).astype(dtype_np)\n",
    "                batch_len = len(arr_batch)\n",
    "                expected_end_idx = idx + batch_len\n",
    "                if expected_end_idx > arr_len: print(f\"Error: Bounds overflow (TS). Stopping.\"); break\n",
    "                arr[idx : expected_end_idx] = arr_batch; idx = expected_end_idx\n",
    "            except Exception as e: print(f\"Error writing TS chunk {i}: {e}\"); break\n",
    "        \n",
    "        \n",
    "        total_samples_bc = len(tokenized_bc)\n",
    "        num_write_batches_bc = math.ceil(total_samples_bc / write_batch_size)\n",
    "        \n",
    "        for i in tqdm(range(num_write_batches_bc), desc=f\"Writing BookCorpus {split}\"):\n",
    "            start = i * write_batch_size; end = min((i + 1) * write_batch_size, total_samples_bc)\n",
    "            chunk = tokenized_bc.select(range(start, end))\n",
    "            try:\n",
    "                all_ids_in_chunk = [item['ids'] for item in chunk if item['ids']]\n",
    "                if not all_ids_in_chunk: continue\n",
    "                arr_batch = np.concatenate(all_ids_in_chunk).astype(dtype_np)\n",
    "                batch_len = len(arr_batch)\n",
    "                expected_end_idx = idx + batch_len\n",
    "                if expected_end_idx > arr_len: print(f\"Error: Bounds overflow (BC). Stopping.\"); break\n",
    "                arr[idx : expected_end_idx] = arr_batch; idx = expected_end_idx\n",
    "            except Exception as e: print(f\"Error writing BC chunk {i}: {e}\"); break\n",
    "\n",
    "        arr.flush()\n",
    "        if idx != arr_len:\n",
    "             print(f\"Warning: Final index {idx} doesn't match expected {arr_len}.\")\n",
    "        else:\n",
    "            print(f\"Successfully wrote {idx} total tokens.\")\n",
    "        print(f\"Finished writing {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memap files\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "train_data_file = os.path.join(config.data_dir, 'train_combined.bin')\n",
    "val_data_file = os.path.join(config.data_dir, 'val_combined.bin')\n",
    "if not os.path.exists(train_data_file) or not os.path.exists(val_data_file):\n",
    "     raise FileNotFoundError(f\"Combined .bin files not found in {config.data_dir}. Did Cell 3 complete?\")\n",
    "\n",
    "train_data = np.memmap(train_data_file, dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(val_data_file, dtype=np.uint16, mode='r')\n",
    "\n",
    "print(f\"Train data loaded: {len(train_data):,} tokens\")\n",
    "print(f\"Validation data loaded: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb207c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
