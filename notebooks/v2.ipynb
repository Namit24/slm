{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6549af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "!pip install -U datasets tiktoken tqdm numpy torch matplotlib huggingface_hub\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization implementation\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import tiktoken \n",
    "import math\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot_token = enc.eot_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fda9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_bpe(sample):\n",
    "    text_data = sample.get('text', '')\n",
    "    if not isinstance(text_data, str): text_data = \"\"\n",
    "    ids = enc.encode_ordinary(text_data)\n",
    "    ids.append(eot_token)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "combined_data_dir = config.data_dir\n",
    "os.makedirs(combined_data_dir, exist_ok=True)\n",
    "train_filename = os.path.join(combined_data_dir, 'train_combined.bin')\n",
    "val_filename = os.path.join(combined_data_dir, 'val_combined.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer with data splits..\n",
    "if os.path.exists(train_filename) and os.path.exists(val_filename):\n",
    "    print(f\"Combined .bin files already exist in {combined_data_dir}. Skipping processing.\")\n",
    "\n",
    "else:\n",
    "    ts_df = load_dataset(\"roneneldan/TinyStories\")\n",
    "    bc_df = load_dataset(\"rojagtap/bookcorpus\", split='train')\n",
    "    \n",
    "    ts_split = ts_df['train'].train_test_split(test_size=0.01, seed=42)\n",
    "    ts_train_df = ts_split['train']\n",
    "    ts_val_df = ts_split['test']\n",
    "    \n",
    "    bc_split = bc_df.train_test_split(test_size=0.01, seed=42)\n",
    "    bc_train_df = bc_split['train']\n",
    "    bc_val_df = bc_split['test']\n",
    "\n",
    "    dataset_splits = {\n",
    "        'train': {'ts': ts_train_df, 'bc': bc_train_df},\n",
    "        'validation': {'ts': ts_val_df, 'bc': bc_val_df}\n",
    "    }\n",
    "\n",
    "    for split in ['train', 'validation']:\n",
    "        filename = train_filename if split == 'train' else val_filename\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "        tokenized_ts = dataset_splits[split]['ts'].map(\n",
    "            processing_bpe, remove_columns=['text'],\n",
    "            desc=f\"Tokenizing TinyStories {split}\", num_proc=config.num_proc\n",
    "        )\n",
    "        \n",
    "        tokenized_bc = dataset_splits[split]['bc'].map(\n",
    "            processing_bpe, remove_columns=['text'],\n",
    "            desc=f\"Tokenizing BookCorpus {split}\", num_proc=config.num_proc\n",
    "        )\n",
    "\n",
    "        ts_len = np.sum(tokenized_ts['len'], dtype=np.uint64)\n",
    "        bc_len = np.sum(tokenized_bc['len'], dtype=np.uint64)\n",
    "        arr_len = ts_len + bc_len\n",
    "\n",
    "        if arr_len == 0:\n",
    "            print(f\"Warning: No tokens found for combined {split} split. Skipping.\"); continue\n",
    "            \n",
    "        print(f\"Total tokens for {split}: {arr_len:,} (TinyStories: {ts_len:,}, BookCorpus: {bc_len:,})\")\n",
    "        dtype_np = np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype_np, mode='w+', shape=(arr_len,))\n",
    "        \n",
    "        print(f\"Writing {ts_len:,} TinyStories tokens to {filename}...\")\n",
    "        idx = 0\n",
    "        write_batch_size = 1000 # Docs per chunk\n",
    "        \n",
    "        total_samples_ts = len(tokenized_ts)\n",
    "        num_write_batches_ts = math.ceil(total_samples_ts / write_batch_size)\n",
    "        for i in tqdm(range(num_write_batches_ts), desc=f\"Writing TinyStories {split}\"):\n",
    "            start = i * write_batch_size; end = min((i + 1) * write_batch_size, total_samples_ts)\n",
    "            chunk = tokenized_ts.select(range(start, end))\n",
    "            try:\n",
    "                all_ids_in_chunk = [item['ids'] for item in chunk if item['ids']]\n",
    "                if not all_ids_in_chunk: continue\n",
    "                arr_batch = np.concatenate(all_ids_in_chunk).astype(dtype_np)\n",
    "                batch_len = len(arr_batch)\n",
    "                expected_end_idx = idx + batch_len\n",
    "                if expected_end_idx > arr_len: print(f\"Error: Bounds overflow (TS). Stopping.\"); break\n",
    "                arr[idx : expected_end_idx] = arr_batch; idx = expected_end_idx\n",
    "            except Exception as e: print(f\"Error writing TS chunk {i}: {e}\"); break\n",
    "        \n",
    "        \n",
    "        total_samples_bc = len(tokenized_bc)\n",
    "        num_write_batches_bc = math.ceil(total_samples_bc / write_batch_size)\n",
    "        \n",
    "        for i in tqdm(range(num_write_batches_bc), desc=f\"Writing BookCorpus {split}\"):\n",
    "            start = i * write_batch_size; end = min((i + 1) * write_batch_size, total_samples_bc)\n",
    "            chunk = tokenized_bc.select(range(start, end))\n",
    "            try:\n",
    "                all_ids_in_chunk = [item['ids'] for item in chunk if item['ids']]\n",
    "                if not all_ids_in_chunk: continue\n",
    "                arr_batch = np.concatenate(all_ids_in_chunk).astype(dtype_np)\n",
    "                batch_len = len(arr_batch)\n",
    "                expected_end_idx = idx + batch_len\n",
    "                if expected_end_idx > arr_len: print(f\"Error: Bounds overflow (BC). Stopping.\"); break\n",
    "                arr[idx : expected_end_idx] = arr_batch; idx = expected_end_idx\n",
    "            except Exception as e: print(f\"Error writing BC chunk {i}: {e}\"); break\n",
    "\n",
    "        arr.flush()\n",
    "        if idx != arr_len:\n",
    "             print(f\"Warning: Final index {idx} doesn't match expected {arr_len}.\")\n",
    "        else:\n",
    "            print(f\"Successfully wrote {idx} total tokens.\")\n",
    "        print(f\"Finished writing {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memap files\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "train_data_file = os.path.join(config.data_dir, 'train_combined.bin')\n",
    "val_data_file = os.path.join(config.data_dir, 'val_combined.bin')\n",
    "if not os.path.exists(train_data_file) or not os.path.exists(val_data_file):\n",
    "     raise FileNotFoundError(f\"Combined .bin files not found in {config.data_dir}. Did Cell 3 complete?\")\n",
    "\n",
    "train_data = np.memmap(train_data_file, dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(val_data_file, dtype=np.uint16, mode='r')\n",
    "\n",
    "print(f\"Train data loaded: {len(train_data):,} tokens\")\n",
    "print(f\"Validation data loaded: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch function\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.block_size, (config.batch_size,))\n",
    "    \n",
    "    x = torch.stack([torch.from_numpy((data[i:i+config.block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+config.block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    if config.device == 'cuda':\n",
    "        x, y = x.pin_memory().to(config.device, non_blocking=True), y.pin_memory().to(config.device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y\n",
    "\n",
    "try:\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    \n",
    "    x_val, y_val = get_batch('val')\n",
    "    print(f\"Validation batch x shape: {x_val.shape}\")\n",
    "    print(f\"Validation batch y shape: {y_val.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing get_batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass, field # Import field\n",
    "\n",
    "@dataclass\n",
    "class SLMConfig:\n",
    "    block_size: int = 256; vocab_size: int = 50257\n",
    "    n_layer: int = 12; n_head: int = 12; n_embd: int = 768\n",
    "    dropout: float = 0.1; bias: bool = False\n",
    "    # Add other fields with defaults from Cell 2 if needed\n",
    "    batch_size: int = 8; gradient_accumulation_steps: int = 4\n",
    "    max_iters: int = 100000; eval_interval: int = 1000\n",
    "    eval_iters: int = 200; learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.1; beta1: float = 0.9; beta2: float = 0.95\n",
    "    grad_clip: float = 1.0; warmup_iters: int = 2000\n",
    "    lr_decay_iters: int = 100000; min_lr: float = 3e-5\n",
    "    device: str = 'cuda'; dtype: str = 'bfloat16'; compile: bool = True\n",
    "    data_dir: str = 'data_combined'; num_proc: int = 8; total_batches: int = 1024\n",
    "    out_dir: str = 'out_v3'; best_model_name: str = 'best_model_v3.pt'\n",
    "    local_pretrained_path: str = \"\"\n",
    "\n",
    "if 'config' not in locals(): raise NameError(\"Config object 'config' not found. Please re-run Cell 2.\")\n",
    "if 'ptdtype' not in locals(): ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[config.dtype]\n",
    "ctx = nullcontext() if config.device == 'cpu' else torch.amp.autocast(device_type=config.device, dtype=ptdtype)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: Flash Attention 2.0 not available.\")\n",
    "        # Causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        is_training = self.training\n",
    "        dropout_val = self.dropout if is_training else 0.0\n",
    "\n",
    "        if self.flash and not is_training:\n",
    "             try: y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
    "             except Exception: self.flash = False; \n",
    "        if not self.flash or is_training:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            current_T = min(T, self.bias.size(-1))\n",
    "            att = att.masked_fill(self.bias[:,:,:current_T,:current_T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1);\n",
    "            if dropout_val > 0.0: att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        proj_output = self.c_proj(y)\n",
    "        if dropout_val > 0.0: y = self.resid_dropout(proj_output)\n",
    "        else: y = proj_output\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x); x = self.gelu(x); x = self.c_proj(x)\n",
    "        if self.training: x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(); self.ln_1 = LayerNorm(config.n_embd, bias=config.bias); self.attn = CausalSelfAttention(config); self.ln_2 = LayerNorm(config.n_embd, bias=config.bias); self.mlp = MLP(config)\n",
    "    def forward(self, x): x = x + self.attn(self.ln_1(x)); x = x + self.mlp(self.ln_2(x)); return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None; assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight # Weight tying\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device; b, t = idx.size();\n",
    "        if t > self.config.block_size: idx = idx[:, -self.config.block_size:]; t = self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "        tok_emb = self.transformer.wte(idx); pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb) if self.training else tok_emb + pos_emb\n",
    "        for block in self.transformer.h: x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else: logits = self.lm_head(x[:, [-1], :]); loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None: v, _ = torch.topk(logits, min(top_k, logits.size(-1))); logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1); idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            if idx_next == enc.eot_token: break\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = GPT(config) \n",
    "\n",
    "model.to(config.device)\n",
    "\n",
    "if config.compile:\n",
    "    print(\"compiling\")\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print(\"Model compiled.\")\n",
    "    except Exception as e:\n",
    "        print(f\"torch.compile failed: {e}. Proceeding without compilation.\")\n",
    "        config.compile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam optimizer and lr scheduler\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "if 'model' not in locals():\n",
    "    raise NameError(\"Model not defined. Please re-run Cell 5.\")\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "for pn, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        if p.dim() >= 2:\n",
    "            decay_params.append(p)\n",
    "        else:\n",
    "            no_decay_params.append(p)\n",
    "\n",
    "param_groups = [\n",
    "    {'params': decay_params, 'weight_decay': config.weight_decay},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "]\n",
    "num_decay_params = sum(p.numel() for p in decay_params)\n",
    "num_nodecay_params = sum(p.numel() for p in no_decay_params)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    param_groups,\n",
    "    lr=config.learning_rate, \n",
    "    betas=(config.beta1, config.beta2),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "def get_lr_multiplier(it):\n",
    "    if it < config.warmup_iters:\n",
    "        return float(it) / float(max(1, config.warmup_iters))\n",
    "    if it > config.lr_decay_iters:\n",
    "        return config.min_lr / config.learning_rate\n",
    "    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    multiplier = (config.min_lr + coeff * (config.learning_rate - config.min_lr)) / config.learning_rate\n",
    "    return multiplier\n",
    "\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=get_lr_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a94aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop with gradient accumulation\n",
    "\n",
    "\n",
    "if 'get_batch' not in globals():\n",
    "     raise NameError(\"get_batch function not defined. Please run Cell 4.\")\n",
    "if 'config' not in locals() or 'ptdtype' not in locals() or 'ctx' not in locals():\n",
    "     raise NameError(\"'config', 'ptdtype', or 'ctx' not defined. Run previous cells.\")\n",
    "if 'optimizer' not in locals() or 'lr_scheduler' not in locals():\n",
    "     raise NameError(\"'optimizer' or 'lr_scheduler' not defined. Run Cell 6.\")\n",
    "if 'estimate_loss' not in globals():\n",
    "     raise NameError(\"'estimate_loss' not defined. Run Cell 7.\")\n",
    "\n",
    "gradient_accumulation_steps = config.gradient_accumulation_steps\n",
    "if gradient_accumulation_steps <= 0:\n",
    "    gradient_accumulation_steps = 1 # Ensure it's at least 1\n",
    "print(f\"Using gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(config.dtype == 'float16'))\n",
    "\n",
    "train_loss_list_v3 = []\n",
    "val_loss_list_v3 = []\n",
    "initial_val_loss = initial_losses.get('val', torch.tensor(float('inf')))\n",
    "best_val_loss = initial_val_loss.item() if not torch.isnan(initial_val_loss) else float('inf')\n",
    "\n",
    "start_time = time.time()\n",
    "# Initialize optimizer gradient just in case\n",
    "optimizer.zero_grad(set_to_none=True) \n",
    "\n",
    "for it in range(config.max_iters):\n",
    "\n",
    "    # --- Periodically Evaluate ---\n",
    "    if it % config.eval_interval == 0 or it == config.max_iters - 1 or it == 0:\n",
    "        actual_eval_iters = config.eval_iters\n",
    "        losses = estimate_loss(model, actual_eval_iters, config.device, ctx)\n",
    "        current_time = time.time(); elapsed_time = current_time - start_time\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        train_loss_val = losses.get('train', torch.tensor(float('nan')))\n",
    "        val_loss_val = losses.get('val', torch.tensor(float('nan')))\n",
    "        train_loss_str = f\"{train_loss_val:.4f}\" if not torch.isnan(train_loss_val) else \"NaN\"\n",
    "        val_loss_str = f\"{val_loss_val:.4f}\" if not torch.isnan(val_loss_val) else \"NaN\"\n",
    "\n",
    "        print(f\"Step {it:6d} | Train Loss: {train_loss_str} | Val Loss: {val_loss_str} | LR: {current_lr:.6f} | Time: {elapsed_time:.2f}s\")\n",
    "\n",
    "        if not torch.isnan(train_loss_val): train_loss_list_v3.append(train_loss_val.item())\n",
    "        if not torch.isnan(val_loss_val): val_loss_list_v3.append(val_loss_val.item())\n",
    "\n",
    "        if not torch.isnan(val_loss_val) and val_loss_val.item() < best_val_loss:\n",
    "            best_val_loss = val_loss_val.item()\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'config': asdict(config), # Save config as dict\n",
    "                'iter': it,\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }\n",
    "            print(f\"  -> Val loss improved from {best_val_loss:.4f} to {val_loss_val.item():.4f}. Saving checkpoint...\")\n",
    "            try:\n",
    "                torch.save(checkpoint, best_model_path) # best_model_path from Cell 2\n",
    "                print(f\"   Checkpoint saved to {best_model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Error saving checkpoint: {e}\")\n",
    "\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        try:\n",
    "            X, Y = get_batch('train')\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting batch at step {it}, micro_step {micro_step}: {e}\"); continue\n",
    "        \n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps \n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            continue \n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if config.grad_clip > 0.0:\n",
    "        scaler.unscale_(optimizer) # Unscale before clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"\\n--- V3 Pre-training Complete ---\")\n",
    "print(f\"Total time: {(end_time - start_time)/60:.2f} minutes\")\n",
    "if best_val_loss == float('inf'): print(\"No valid best validation loss recorded.\")\n",
    "else: print(f\"Best validation loss achieved: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00db02",
   "metadata": {},
   "source": [
    "Using gradient accumulation steps: 4\n",
    "Initializing best_val_loss to: 11.0000\n",
    "Starting training for 100000 iterations...\n",
    "Step      0 | Train Loss: 11.0000 | Val Loss: 11.0000 | LR: 0.000000 | Time: 22.22s\n",
    "Step   1000 | Train Loss: 4.5938 | Val Loss: 4.5938 | LR: 0.000150 | Time: 842.09s\n",
    "  -> Val loss improved from 4.5938 to 4.5938. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step   2000 | Train Loss: 3.9844 | Val Loss: 4.0312 | LR: 0.000300 | Time: 1656.08s\n",
    "  -> Val loss improved from 4.0312 to 4.0312. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step   3000 | Train Loss: 3.7656 | Val Loss: 3.7188 | LR: 0.000300 | Time: 2477.06s\n",
    "  -> Val loss improved from 3.7188 to 3.7188. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step   4000 | Train Loss: 3.5781 | Val Loss: 3.5938 | LR: 0.000300 | Time: 3295.82s\n",
    "  -> Val loss improved from 3.5938 to 3.5938. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step   5000 | Train Loss: 3.5312 | Val Loss: 3.5000 | LR: 0.000299 | Time: 4121.51s\n",
    "  -> Val loss improved from 3.5000 to 3.5000. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step   6000 | Train Loss: 3.4688 | Val Loss: 3.4531 | LR: 0.000299 | Time: 4945.68s\n",
    "  -> Val loss improved from 3.4531 to 3.4531. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step   7000 | Train Loss: 3.4531 | Val Loss: 3.4531 | LR: 0.000298 | Time: 5773.21s\n",
    "Step   8000 | Train Loss: 3.4062 | Val Loss: 3.3906 | LR: 0.000298 | Time: 6578.49s\n",
    "  -> Val loss improved from 3.3906 to 3.3906. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step   9000 | Train Loss: 3.3594 | Val Loss: 3.3594 | LR: 0.000297 | Time: 7390.34s\n",
    "  -> Val loss improved from 3.3594 to 3.3594. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  10000 | Train Loss: 3.3125 | Val Loss: 3.3594 | LR: 0.000296 | Time: 8201.33s\n",
    "Step  11000 | Train Loss: 3.2812 | Val Loss: 3.3281 | LR: 0.000294 | Time: 9010.95s\n",
    "  -> Val loss improved from 3.3281 to 3.3281. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  12000 | Train Loss: 3.3438 | Val Loss: 3.2656 | LR: 0.000293 | Time: 9795.86s\n",
    "  -> Val loss improved from 3.2656 to 3.2656. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  13000 | Train Loss: 3.2656 | Val Loss: 3.2812 | LR: 0.000292 | Time: 10595.66s\n",
    "Step  14000 | Train Loss: 3.2500 | Val Loss: 3.2188 | LR: 0.000290 | Time: 11485.11s\n",
    "  -> Val loss improved from 3.2188 to 3.2188. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  15000 | Train Loss: 3.2969 | Val Loss: 3.2656 | LR: 0.000288 | Time: 12460.12s\n",
    "Step  16000 | Train Loss: 3.2344 | Val Loss: 3.2031 | LR: 0.000287 | Time: 13415.72s\n",
    "  -> Val loss improved from 3.2031 to 3.2031. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  17000 | Train Loss: 3.2344 | Val Loss: 3.2344 | LR: 0.000285 | Time: 14289.98s\n",
    "Step  18000 | Train Loss: 3.2031 | Val Loss: 3.1719 | LR: 0.000283 | Time: 15160.89s\n",
    "  -> Val loss improved from 3.1719 to 3.1719. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  19000 | Train Loss: 3.1875 | Val Loss: 3.2031 | LR: 0.000280 | Time: 16024.75s\n",
    "Step  20000 | Train Loss: 3.2031 | Val Loss: 3.2031 | LR: 0.000278 | Time: 16941.32s\n",
    "Step  21000 | Train Loss: 3.2031 | Val Loss: 3.1719 | LR: 0.000276 | Time: 17906.12s\n",
    "Step  22000 | Train Loss: 3.1406 | Val Loss: 3.1719 | LR: 0.000273 | Time: 18898.45s\n",
    "Step  23000 | Train Loss: 3.1406 | Val Loss: 3.1719 | LR: 0.000271 | Time: 19861.49s\n",
    "Step  24000 | Train Loss: 3.1562 | Val Loss: 3.1406 | LR: 0.000268 | Time: 20825.00s\n",
    "  -> Val loss improved from 3.1406 to 3.1406. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  25000 | Train Loss: 3.1562 | Val Loss: 3.1562 | LR: 0.000265 | Time: 21804.86s\n",
    "Step  26000 | Train Loss: 3.0938 | Val Loss: 3.1562 | LR: 0.000262 | Time: 22804.84s\n",
    "Step  27000 | Train Loss: 3.1875 | Val Loss: 3.1250 | LR: 0.000259 | Time: 23808.41s\n",
    "  -> Val loss improved from 3.1250 to 3.1250. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  28000 | Train Loss: 3.1250 | Val Loss: 3.1406 | LR: 0.000256 | Time: 24828.07s\n",
    "Step  29000 | Train Loss: 3.1250 | Val Loss: 3.1250 | LR: 0.000253 | Time: 25797.07s\n",
    "Step  30000 | Train Loss: 3.0781 | Val Loss: 3.0781 | LR: 0.000249 | Time: 26805.60s\n",
    "  -> Val loss improved from 3.0781 to 3.0781. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  31000 | Train Loss: 3.1406 | Val Loss: 3.1094 | LR: 0.000246 | Time: 27896.45s\n",
    "Step  32000 | Train Loss: 3.0938 | Val Loss: 3.0781 | LR: 0.000242 | Time: 28859.89s\n",
    "Step  33000 | Train Loss: 3.1250 | Val Loss: 3.0781 | LR: 0.000239 | Time: 29629.99s\n",
    "Step  34000 | Train Loss: 3.0781 | Val Loss: 3.0938 | LR: 0.000235 | Time: 30402.28s\n",
    "Step  35000 | Train Loss: 3.1250 | Val Loss: 3.0781 | LR: 0.000231 | Time: 31175.06s\n",
    "Step  36000 | Train Loss: 3.1094 | Val Loss: 3.0938 | LR: 0.000227 | Time: 31947.80s\n",
    "Step  37000 | Train Loss: 3.0625 | Val Loss: 3.0938 | LR: 0.000224 | Time: 32721.14s\n",
    "Step  38000 | Train Loss: 3.0625 | Val Loss: 3.0625 | LR: 0.000220 | Time: 33494.00s\n",
    "  -> Val loss improved from 3.0625 to 3.0625. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  39000 | Train Loss: 3.1094 | Val Loss: 3.0781 | LR: 0.000216 | Time: 34268.12s\n",
    "Step  40000 | Train Loss: 3.0625 | Val Loss: 3.1250 | LR: 0.000212 | Time: 35042.76s\n",
    "Step  41000 | Train Loss: 3.0312 | Val Loss: 3.0781 | LR: 0.000208 | Time: 35817.89s\n",
    "Step  42000 | Train Loss: 3.0469 | Val Loss: 3.0625 | LR: 0.000203 | Time: 36594.54s\n",
    "Step  43000 | Train Loss: 3.0312 | Val Loss: 3.0469 | LR: 0.000199 | Time: 37369.47s\n",
    "  -> Val loss improved from 3.0469 to 3.0469. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  44000 | Train Loss: 3.0469 | Val Loss: 3.0938 | LR: 0.000195 | Time: 38147.14s\n",
    "Step  45000 | Train Loss: 3.0469 | Val Loss: 3.0781 | LR: 0.000191 | Time: 38928.20s\n",
    "Step  46000 | Train Loss: 3.0781 | Val Loss: 3.0781 | LR: 0.000187 | Time: 39708.15s\n",
    "Step  47000 | Train Loss: 3.0469 | Val Loss: 3.0781 | LR: 0.000182 | Time: 40487.42s\n",
    "Step  48000 | Train Loss: 3.0156 | Val Loss: 3.1094 | LR: 0.000178 | Time: 41269.54s\n",
    "Step  49000 | Train Loss: 3.0312 | Val Loss: 3.0469 | LR: 0.000174 | Time: 42046.33s\n",
    "Step  50000 | Train Loss: 2.9844 | Val Loss: 3.0469 | LR: 0.000169 | Time: 42821.40s\n",
    "Step  51000 | Train Loss: 3.0469 | Val Loss: 3.0781 | LR: 0.000165 | Time: 43598.10s\n",
    "Step  52000 | Train Loss: 3.0312 | Val Loss: 3.0312 | LR: 0.000161 | Time: 44377.59s\n",
    "  -> Val loss improved from 3.0312 to 3.0312. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  53000 | Train Loss: 3.0000 | Val Loss: 3.0312 | LR: 0.000156 | Time: 45158.59s\n",
    "Step  54000 | Train Loss: 3.0156 | Val Loss: 3.0469 | LR: 0.000152 | Time: 45941.50s\n",
    "Step  55000 | Train Loss: 3.0156 | Val Loss: 3.0312 | LR: 0.000148 | Time: 46725.17s\n",
    "Step  56000 | Train Loss: 3.0312 | Val Loss: 3.0781 | LR: 0.000143 | Time: 47507.60s\n",
    "Step  57000 | Train Loss: 3.0625 | Val Loss: 3.0156 | LR: 0.000139 | Time: 48292.61s\n",
    "  -> Val loss improved from 3.0156 to 3.0156. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  58000 | Train Loss: 3.0156 | Val Loss: 3.0312 | LR: 0.000135 | Time: 49079.20s\n",
    "Step  59000 | Train Loss: 3.0156 | Val Loss: 3.0156 | LR: 0.000131 | Time: 49861.42s\n",
    "Step  60000 | Train Loss: 3.0469 | Val Loss: 3.0469 | LR: 0.000127 | Time: 50640.45s\n",
    "Step  61000 | Train Loss: 3.0156 | Val Loss: 3.0625 | LR: 0.000122 | Time: 51417.22s\n",
    "Step  62000 | Train Loss: 3.0000 | Val Loss: 3.0312 | LR: 0.000118 | Time: 52194.85s\n",
    "Step  63000 | Train Loss: 3.0000 | Val Loss: 2.9688 | LR: 0.000114 | Time: 52970.24s\n",
    "  -> Val loss improved from 2.9688 to 2.9688. Saving checkpoint...\n",
    "   Checkpoint saved to out_v3/best_model_v3.pt\n",
    "Step  64000 | Train Loss: 3.0000 | Val Loss: 3.0469 | LR: 0.000110 | Time: 53748.13s\n",
    "Step  65000 | Train Loss: 3.0312 | Val Loss: 2.9844 | LR: 0.000106 | Time: 54523.57s\n",
    "Step  66000 | Train Loss: 3.0469 | Val Loss: 3.0312 | LR: 0.000103 | Time: 55300.06s\n",
    "Step  67000 | Train Loss: 3.0469 | Val Loss: 3.0000 | LR: 0.000099 | Time: 56076.09s\n",
    "Step  68000 | Train Loss: 2.9844 | Val Loss: 3.0000 | LR: 0.000095 | Time: 56852.66s\n",
    "Step  69000 | Train Loss: 3.0000 | Val Loss: 3.0469 | LR: 0.000091 | Time: 57629.19s\n",
    "Step  70000 | Train Loss: 3.0625 | Val Loss: 3.0312 | LR: 0.000088 | Time: 58404.47s\n",
    "Step  71000 | Train Loss: 3.0000 | Val Loss: 3.0312 | LR: 0.000084 | Time: 59179.44s\n",
    "Step  72000 | Train Loss: 3.0312 | Val Loss: 3.0156 | LR: 0.000081 | Time: 59955.59s\n",
    "Step  73000 | Train Loss: 3.0469 | Val Loss: 3.0156 | LR: 0.000077 | Time: 60730.39s\n",
    "Step  74000 | Train Loss: 2.9844 | Val Loss: 2.9688 | LR: 0.000074 | Time: 61504.59s\n",
    "Step  75000 | Train Loss: 2.9844 | Val Loss: 2.9844 | LR: 0.000071 | Time: 62279.40s\n",
    "Step  76000 | Train Loss: 2.9844 | Val Loss: 3.0156 | LR: 0.000068 | Time: 63052.62s\n",
    "Step  77000 | Train Loss: 3.0156 | Val Loss: 3.0156 | LR: 0.000065 | Time: 63825.37s\n",
    "Step  78000 | Train Loss: 3.0312 | Val Loss: 3.0000 | LR: 0.000062 | Time: 64598.61s\n",
    "Step  79000 | Train Loss: 3.0469 | Val Loss: 2.9844 | LR: 0.000059 | Time: 65371.89s\n",
    "Step  80000 | Train Loss: 3.0000 | Val Loss: 3.0469 | LR: 0.000057 | Time: 66145.94s\n",
    "Step  81000 | Train Loss: 2.9688 | Val Loss: 2.9844 | LR: 0.000054 | Time: 66919.85s\n",
    "Step  82000 | Train Loss: 2.9688 | Val Loss: 3.0469 | LR: 0.000052 | Time: 67693.88s\n",
    "Step  83000 | Train Loss: 3.0312 | Val Loss: 3.0312 | LR: 0.000050 | Time: 68469.56s\n",
    "Step  84000 | Train Loss: 3.0000 | Val Loss: 3.0312 | LR: 0.000047 | Time: 69242.97s\n",
    "Step  85000 | Train Loss: 3.0312 | Val Loss: 3.0156 | LR: 0.000045 | Time: 70016.58s\n",
    "Step  86000 | Train Loss: 3.0000 | Val Loss: 3.0156 | LR: 0.000043 | Time: 70790.21s\n",
    "Step  87000 | Train Loss: 2.9688 | Val Loss: 3.0312 | LR: 0.000042 | Time: 71562.06s\n",
    "Step  88000 | Train Loss: 3.0156 | Val Loss: 3.0000 | LR: 0.000040 | Time: 72333.70s\n",
    "Step  89000 | Train Loss: 3.0000 | Val Loss: 2.9688 | LR: 0.000038 | Time: 73106.02s\n",
    "Step  90000 | Train Loss: 3.0625 | Val Loss: 2.9688 | LR: 0.000037 | Time: 73874.55s\n",
    "Step  91000 | Train Loss: 3.0312 | Val Loss: 3.0312 | LR: 0.000036 | Time: 74641.63s\n",
    "Step  92000 | Train Loss: 3.0000 | Val Loss: 2.9844 | LR: 0.000034 | Time: 75411.81s\n",
    "Step  93000 | Train Loss: 2.9688 | Val Loss: 3.0000 | LR: 0.000033 | Time: 76187.39s\n",
    "Step  94000 | Train Loss: 3.0000 | Val Loss: 3.0000 | LR: 0.000032 | Time: 76960.16s\n",
    "Step  95000 | Train Loss: 2.9844 | Val Loss: 3.0312 | LR: 0.000032 | Time: 77728.03s\n",
    "Step  96000 | Train Loss: 3.0000 | Val Loss: 3.0000 | LR: 0.000031 | Time: 78501.28s\n",
    "Step  97000 | Train Loss: 2.9688 | Val Loss: 3.0000 | LR: 0.000031 | Time: 79273.19s\n",
    "Step  98000 | Train Loss: 3.0312 | Val Loss: 3.0000 | LR: 0.000030 | Time: 80046.06s\n",
    "Step  99000 | Train Loss: 2.9844 | Val Loss: 3.0156 | LR: 0.000030 | Time: 80819.30s\n",
    "Step  99999 | Train Loss: 3.0312 | Val Loss: 3.0469 | LR: 0.000030 | Time: 81591.22s\n",
    "\n",
    "--- V3 Pre-training Complete ---\n",
    "Total time: 1359.86 minutes\n",
    "Best validation loss achieved: 2.9688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "if 'train_loss_list_v3' not in locals() or 'val_loss_list_v3' not in locals():\n",
    "     raise NameError(\"Loss lists (train_loss_list_v3/val_loss_list_v3) not found.\")\n",
    "if 'config' not in locals():\n",
    "     raise NameError(\"Config object not found. Please run Cell 2.\")\n",
    "\n",
    "\n",
    "eval_interval = config.eval_interval\n",
    "steps = [i * eval_interval for i in range(len(train_loss_list_v3))]\n",
    "\n",
    "train_losses = train_loss_list_v3[:len(steps)]\n",
    "val_losses = val_loss_list_v3[:len(steps)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(steps, train_losses, 'g-o', label='train loss', markersize=3, alpha=0.7) #\n",
    "plt.plot(steps, val_losses, 'r-o', label='val loss', markersize=3, alpha=0.7)\n",
    "plt.xlabel(f\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "all_losses_except_first = train_losses[1:] + val_losses[1:]\n",
    "if all_losses_except_first:\n",
    "    min_loss = min(all_losses_except_first)\n",
    "    max_loss = max(all_losses_except_first)\n",
    "    plt.ylim(bottom=min_loss - 0.1, top=max_loss + 0.1)\n",
    "else:\n",
    "    min_loss = min(train_losses + val_losses) if (train_losses + val_losses) else 0\n",
    "    max_loss = max(train_losses + val_losses) if (train_losses + val_losses) else 11\n",
    "    plt.ylim(bottom=min_loss - 0.2, top=max_loss + 0.2)\n",
    "    \n",
    "    \n",
    "plt.legend()\n",
    "plt.title(\"V2 Model Pre-training Loss (TinyStories + BookCorpus)\")\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "major_ticks = np.arange(0, config.max_iters + 1, 10000)\n",
    "minor_ticks = np.arange(0, config.max_iters + 1, 5000)\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_filename = os.path.join(config.out_dir, \"loss_plot_v3.png\")\n",
    "try:\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"Loss plot saved to {plot_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving plot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc72cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
