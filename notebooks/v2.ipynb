{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6549af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "!pip install -U datasets tiktoken tqdm numpy torch matplotlib huggingface_hub\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization implementation\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import tiktoken \n",
    "import math\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot_token = enc.eot_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fda9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_bpe(sample):\n",
    "    text_data = sample.get('text', '')\n",
    "    if not isinstance(text_data, str): text_data = \"\"\n",
    "    ids = enc.encode_ordinary(text_data)\n",
    "    ids.append(eot_token)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "combined_data_dir = config.data_dir\n",
    "os.makedirs(combined_data_dir, exist_ok=True)\n",
    "train_filename = os.path.join(combined_data_dir, 'train_combined.bin')\n",
    "val_filename = os.path.join(combined_data_dir, 'val_combined.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer with data splits..\n",
    "if os.path.exists(train_filename) and os.path.exists(val_filename):\n",
    "    print(f\"Combined .bin files already exist in {combined_data_dir}. Skipping processing.\")\n",
    "\n",
    "else:\n",
    "    ts_df = load_dataset(\"roneneldan/TinyStories\")\n",
    "    bc_df = load_dataset(\"rojagtap/bookcorpus\", split='train')\n",
    "    \n",
    "    ts_split = ts_df['train'].train_test_split(test_size=0.01, seed=42)\n",
    "    ts_train_df = ts_split['train']\n",
    "    ts_val_df = ts_split['test']\n",
    "    \n",
    "    bc_split = bc_df.train_test_split(test_size=0.01, seed=42)\n",
    "    bc_train_df = bc_split['train']\n",
    "    bc_val_df = bc_split['test']\n",
    "\n",
    "    dataset_splits = {\n",
    "        'train': {'ts': ts_train_df, 'bc': bc_train_df},\n",
    "        'validation': {'ts': ts_val_df, 'bc': bc_val_df}\n",
    "    }\n",
    "\n",
    "    for split in ['train', 'validation']:\n",
    "        filename = train_filename if split == 'train' else val_filename\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "        tokenized_ts = dataset_splits[split]['ts'].map(\n",
    "            processing_bpe, remove_columns=['text'],\n",
    "            desc=f\"Tokenizing TinyStories {split}\", num_proc=config.num_proc\n",
    "        )\n",
    "        \n",
    "        tokenized_bc = dataset_splits[split]['bc'].map(\n",
    "            processing_bpe, remove_columns=['text'],\n",
    "            desc=f\"Tokenizing BookCorpus {split}\", num_proc=config.num_proc\n",
    "        )\n",
    "\n",
    "        ts_len = np.sum(tokenized_ts['len'], dtype=np.uint64)\n",
    "        bc_len = np.sum(tokenized_bc['len'], dtype=np.uint64)\n",
    "        arr_len = ts_len + bc_len\n",
    "\n",
    "        if arr_len == 0:\n",
    "            print(f\"Warning: No tokens found for combined {split} split. Skipping.\"); continue\n",
    "            \n",
    "        print(f\"Total tokens for {split}: {arr_len:,} (TinyStories: {ts_len:,}, BookCorpus: {bc_len:,})\")\n",
    "        dtype_np = np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype_np, mode='w+', shape=(arr_len,))\n",
    "        \n",
    "        print(f\"Writing {ts_len:,} TinyStories tokens to {filename}...\")\n",
    "        idx = 0\n",
    "        write_batch_size = 1000 # Docs per chunk\n",
    "        \n",
    "        total_samples_ts = len(tokenized_ts)\n",
    "        num_write_batches_ts = math.ceil(total_samples_ts / write_batch_size)\n",
    "        for i in tqdm(range(num_write_batches_ts), desc=f\"Writing TinyStories {split}\"):\n",
    "            start = i * write_batch_size; end = min((i + 1) * write_batch_size, total_samples_ts)\n",
    "            chunk = tokenized_ts.select(range(start, end))\n",
    "            try:\n",
    "                all_ids_in_chunk = [item['ids'] for item in chunk if item['ids']]\n",
    "                if not all_ids_in_chunk: continue\n",
    "                arr_batch = np.concatenate(all_ids_in_chunk).astype(dtype_np)\n",
    "                batch_len = len(arr_batch)\n",
    "                expected_end_idx = idx + batch_len\n",
    "                if expected_end_idx > arr_len: print(f\"Error: Bounds overflow (TS). Stopping.\"); break\n",
    "                arr[idx : expected_end_idx] = arr_batch; idx = expected_end_idx\n",
    "            except Exception as e: print(f\"Error writing TS chunk {i}: {e}\"); break\n",
    "        \n",
    "        \n",
    "        total_samples_bc = len(tokenized_bc)\n",
    "        num_write_batches_bc = math.ceil(total_samples_bc / write_batch_size)\n",
    "        \n",
    "        for i in tqdm(range(num_write_batches_bc), desc=f\"Writing BookCorpus {split}\"):\n",
    "            start = i * write_batch_size; end = min((i + 1) * write_batch_size, total_samples_bc)\n",
    "            chunk = tokenized_bc.select(range(start, end))\n",
    "            try:\n",
    "                all_ids_in_chunk = [item['ids'] for item in chunk if item['ids']]\n",
    "                if not all_ids_in_chunk: continue\n",
    "                arr_batch = np.concatenate(all_ids_in_chunk).astype(dtype_np)\n",
    "                batch_len = len(arr_batch)\n",
    "                expected_end_idx = idx + batch_len\n",
    "                if expected_end_idx > arr_len: print(f\"Error: Bounds overflow (BC). Stopping.\"); break\n",
    "                arr[idx : expected_end_idx] = arr_batch; idx = expected_end_idx\n",
    "            except Exception as e: print(f\"Error writing BC chunk {i}: {e}\"); break\n",
    "\n",
    "        arr.flush()\n",
    "        if idx != arr_len:\n",
    "             print(f\"Warning: Final index {idx} doesn't match expected {arr_len}.\")\n",
    "        else:\n",
    "            print(f\"Successfully wrote {idx} total tokens.\")\n",
    "        print(f\"Finished writing {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memap files\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "train_data_file = os.path.join(config.data_dir, 'train_combined.bin')\n",
    "val_data_file = os.path.join(config.data_dir, 'val_combined.bin')\n",
    "if not os.path.exists(train_data_file) or not os.path.exists(val_data_file):\n",
    "     raise FileNotFoundError(f\"Combined .bin files not found in {config.data_dir}. Did Cell 3 complete?\")\n",
    "\n",
    "train_data = np.memmap(train_data_file, dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(val_data_file, dtype=np.uint16, mode='r')\n",
    "\n",
    "print(f\"Train data loaded: {len(train_data):,} tokens\")\n",
    "print(f\"Validation data loaded: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch function\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.block_size, (config.batch_size,))\n",
    "    \n",
    "    x = torch.stack([torch.from_numpy((data[i:i+config.block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+config.block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    if config.device == 'cuda':\n",
    "        x, y = x.pin_memory().to(config.device, non_blocking=True), y.pin_memory().to(config.device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y\n",
    "\n",
    "try:\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    \n",
    "    x_val, y_val = get_batch('val')\n",
    "    print(f\"Validation batch x shape: {x_val.shape}\")\n",
    "    print(f\"Validation batch y shape: {y_val.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing get_batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model setup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass, field # Import field\n",
    "\n",
    "@dataclass\n",
    "class SLMConfig:\n",
    "    block_size: int = 256; vocab_size: int = 50257\n",
    "    n_layer: int = 12; n_head: int = 12; n_embd: int = 768\n",
    "    dropout: float = 0.1; bias: bool = False\n",
    "    # Add other fields with defaults from Cell 2 if needed\n",
    "    batch_size: int = 8; gradient_accumulation_steps: int = 4\n",
    "    max_iters: int = 100000; eval_interval: int = 1000\n",
    "    eval_iters: int = 200; learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.1; beta1: float = 0.9; beta2: float = 0.95\n",
    "    grad_clip: float = 1.0; warmup_iters: int = 2000\n",
    "    lr_decay_iters: int = 100000; min_lr: float = 3e-5\n",
    "    device: str = 'cuda'; dtype: str = 'bfloat16'; compile: bool = True\n",
    "    data_dir: str = 'data_combined'; num_proc: int = 8; total_batches: int = 1024\n",
    "    out_dir: str = 'out_v3'; best_model_name: str = 'best_model_v3.pt'\n",
    "    local_pretrained_path: str = \"\"\n",
    "\n",
    "if 'config' not in locals(): raise NameError(\"Config object 'config' not found. Please re-run Cell 2.\")\n",
    "if 'ptdtype' not in locals(): ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[config.dtype]\n",
    "ctx = nullcontext() if config.device == 'cpu' else torch.amp.autocast(device_type=config.device, dtype=ptdtype)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: Flash Attention 2.0 not available.\")\n",
    "        # Causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        is_training = self.training\n",
    "        dropout_val = self.dropout if is_training else 0.0\n",
    "\n",
    "        if self.flash and not is_training:\n",
    "             try: y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True)\n",
    "             except Exception: self.flash = False; \n",
    "        if not self.flash or is_training:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            current_T = min(T, self.bias.size(-1))\n",
    "            att = att.masked_fill(self.bias[:,:,:current_T,:current_T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1);\n",
    "            if dropout_val > 0.0: att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        proj_output = self.c_proj(y)\n",
    "        if dropout_val > 0.0: y = self.resid_dropout(proj_output)\n",
    "        else: y = proj_output\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x); x = self.gelu(x); x = self.c_proj(x)\n",
    "        if self.training: x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(); self.ln_1 = LayerNorm(config.n_embd, bias=config.bias); self.attn = CausalSelfAttention(config); self.ln_2 = LayerNorm(config.n_embd, bias=config.bias); self.mlp = MLP(config)\n",
    "    def forward(self, x): x = x + self.attn(self.ln_1(x)); x = x + self.mlp(self.ln_2(x)); return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None; assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight # Weight tying\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device; b, t = idx.size();\n",
    "        if t > self.config.block_size: idx = idx[:, -self.config.block_size:]; t = self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "        tok_emb = self.transformer.wte(idx); pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb) if self.training else tok_emb + pos_emb\n",
    "        for block in self.transformer.h: x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else: logits = self.lm_head(x[:, [-1], :]); loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None: v, _ = torch.topk(logits, min(top_k, logits.size(-1))); logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1); idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            if idx_next == enc.eot_token: break\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = GPT(config) \n",
    "\n",
    "model.to(config.device)\n",
    "\n",
    "if config.compile:\n",
    "    print(\"compiling\")\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print(\"Model compiled.\")\n",
    "    except Exception as e:\n",
    "        print(f\"torch.compile failed: {e}. Proceeding without compilation.\")\n",
    "        config.compile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam optimizer and lr scheduler\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "if 'model' not in locals():\n",
    "    raise NameError(\"Model not defined. Please re-run Cell 5.\")\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "for pn, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        if p.dim() >= 2:\n",
    "            decay_params.append(p)\n",
    "        else:\n",
    "            no_decay_params.append(p)\n",
    "\n",
    "param_groups = [\n",
    "    {'params': decay_params, 'weight_decay': config.weight_decay},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "]\n",
    "num_decay_params = sum(p.numel() for p in decay_params)\n",
    "num_nodecay_params = sum(p.numel() for p in no_decay_params)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    param_groups,\n",
    "    lr=config.learning_rate, \n",
    "    betas=(config.beta1, config.beta2),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "def get_lr_multiplier(it):\n",
    "    if it < config.warmup_iters:\n",
    "        return float(it) / float(max(1, config.warmup_iters))\n",
    "    if it > config.lr_decay_iters:\n",
    "        return config.min_lr / config.learning_rate\n",
    "    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    multiplier = (config.min_lr + coeff * (config.learning_rate - config.min_lr)) / config.learning_rate\n",
    "    return multiplier\n",
    "\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda=get_lr_multiplier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
