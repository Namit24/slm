{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6549af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "!pip install -U datasets tiktoken tqdm numpy torch matplotlib huggingface_hub\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization implementation\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import tiktoken \n",
    "import math\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot_token = enc.eot_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fda9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_bpe(sample):\n",
    "    text_data = sample.get('text', '')\n",
    "    if not isinstance(text_data, str): text_data = \"\"\n",
    "    ids = enc.encode_ordinary(text_data)\n",
    "    ids.append(eot_token)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "combined_data_dir = config.data_dir\n",
    "os.makedirs(combined_data_dir, exist_ok=True)\n",
    "train_filename = os.path.join(combined_data_dir, 'train_combined.bin')\n",
    "val_filename = os.path.join(combined_data_dir, 'val_combined.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data import and division\n",
    "if os.path.exists(train_filename) and os.path.exists(val_filename):\n",
    "    print(f\"Combined .bin files already exist in {combined_data_dir}. Skipping processing.\")\n",
    "\n",
    "else:\n",
    "    ts_df = load_dataset(\"roneneldan/TinyStories\")\n",
    "    bc_df = load_dataset(\"rojagtap/bookcorpus\", split='train')\n",
    "    \n",
    "    #Train/Validation Split\n",
    "    ts_split = ts_df['train'].train_test_split(test_size=0.01, seed=42)\n",
    "    ts_train_df = ts_split['train']\n",
    "    ts_val_df = ts_split['test']\n",
    "    \n",
    "    bc_split = bc_df.train_test_split(test_size=0.01, seed=42)\n",
    "    bc_train_df = bc_split['train']\n",
    "    bc_val_df = bc_split['test']\n",
    "\n",
    "    dataset_splits = {\n",
    "        'train': {'ts': ts_train_df, 'bc': bc_train_df},\n",
    "        'validation': {'ts': ts_val_df, 'bc': bc_val_df}\n",
    "    }\n",
    "\n",
    "    for split in ['train', 'validation']:\n",
    "        filename = train_filename if split == 'train' else val_filename\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            print(f\"Deleting existing {filename} to rebuild...\")\n",
    "            os.remove(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
